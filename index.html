<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>FU Shi-yang|Reasearch Road</title>
  
  <meta name="author" content="FU Shi-yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
		    
              <p style="text-align:center">
                <name>FU Shi-yang</name>
              </p>
              <p>Hi! I'm FU Shi-yang, a student at CUGB in Beijing.
              </p>
              <p>
	        My focus is on creating immersive 3D experiences from easy-to-capture footage of real places. For example, you may want to scan and digitally revisit your childhood home, or capture a VR-ready 3D panorama of the places you visit during a vacation. 
	      </p>
	      <p>
	        Formally, my research interests are view synthesis, image-based rendering, neural rendering and real-time graphics.
	      </p>
		    
		
              <p style="text-align:center">
                <a href="mailto:ifushiyang@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/ffushiyang-CV.pdf">CV</a> &nbsp/&nbsp
		      
                <!--
		      <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp
		      -->
		      
                <a href="https://github.com/ffushiyang/">Github</a>
              </p>
            </td>
	    
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ffushiyang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ffushiyang_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <!--I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              	-->
	      </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!--第一个内容块-->
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/1.jpg" alt="1" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/128LjDgrlQ6nIGtwGhD5CGJjQafufj91H/view?usp=drive_link">
                <papertitle>顾及分段表达的中国区域ZWD模型</papertitle>
              </a>
              <br>
              <strong>黄宁</strong>, <a href="https://www.researchgate.net/profile/Ning-Huang-33">付世洋</a>, <a href="https://www.researchgate.net/profile/Shiyang-Fu">黄良珂</a>, <a href="https://www.researchgate.net/profile/Liangke-Huang">et al</a>
              <br>		    
              <em>测绘通报</em>, 2023
              <br>
              <a href="data/hnZWD2023.bib">bibtex</a>
              <p>建立了一种顾及分段表达的中国区域ZWD模型（CZWD模型）实验表明，精度优于目前应用较广的GPT3模型，且提高了5%，在中国区域总体上显示出较优的精度和适用性。
            </td>
          </tr>

		
<!--第一个内容块-->
    <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bakedsdf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bakedsdf_start() {
            document.getElementById('bakedsdf_image').style.opacity = "1";
          }

          function bakedsdf_stop() {
            document.getElementById('bakedsdf_image').style.opacity = "0";
          }
          bakedsdf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://bakedsdf.github.io/">
          <papertitle>BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</papertitle>
        </a>
        <br>
        <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
        <a href="https://phogzone.com/">Peter Hedman*</a>,
        <a href="https://creiser.github.io/">Christian Reiser</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,  <br>
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>SIGGRAPH</em>, 2023
        <br>
        <a href="https://bakedsdf.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
        /
        <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
        <p></p>
        <p>
        We use SDFs to bake a NeRF-like model into a high quality mesh and do real-time view synthesis.
        </p>
      </td>
    </tr>



<!--第二个内容块-->
    <tr onmouseout="merf_stop()" onmouseover="merf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/merf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/merf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function merf_start() {
            document.getElementById('merf_image').style.opacity = "1";
          }

          function merf_stop() {
            document.getElementById('merf_image').style.opacity = "0";
          }
          merf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://merf42.github.io/">
          <papertitle>MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</papertitle>
        </a>
        <br>
        <a href="https://creiser.github.io/">Christian Reiser</a>,
        <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>SIGGRAPH</em>, 2023
        <br>
        <a href="https://merf42.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
        /
        <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
        <p></p>
        <p>
        We use volumetric rendering with a sparse 3D feature grid and 2D feature planes to do real-time view synthesis.
        </p>
      </td>
    </tr>

<!--第三个内容块-->
    <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/eclipse_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/eclipse_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function eclipse_start() {
            document.getElementById('eclipse_image').style.opacity = "1";
          }

          function eclipse_stop() {
            document.getElementById('eclipse_image').style.opacity = "0";
          }
          eclipse_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dorverbin.github.io/eclipse">
          <papertitle>Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</papertitle>
        </a>
        <br>
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>, <br>
        <strong>Jonathan T. Barron</strong>,
        <a href="Todd Zickler">Todd Zickler</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://dorverbin.github.io/eclipse">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
        /
        <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
        <p></p>
        <p>
        Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
        </p>
      </td>
    </tr>

<!--第四个内容块-->
    <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/zipnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/zipnerf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function zipnerf_start() {
            document.getElementById('zipnerf_image').style.opacity = "1";
          }

          function zipnerf_stop() {
            document.getElementById('zipnerf_image').style.opacity = "0";
          }
          zipnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://ffushiyang.github.io/zipnerf">
          <papertitle>Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</papertitle>
        </a>
        <br>
        <strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="http://ffushiyang.github.io/zipnerf">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
        /
        <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
        <p></p>
        <p>
        Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
        </p>
      </td>
    </tr>


<!--第五个内容块-->
	 <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mipnerf_ipe_yellow.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://ffushiyang.github.io/mipnerf">
                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://ffushiyang.github.io/mipnerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
              /
              <a href="https://youtu.be/EpH175PY1A0">video</a>
							/
              <a href="https://github.com/google/mipnerf">code</a>
              <p></p>
              <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
            </td>
          </tr> 

<!--第六个内容块-->
	<tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mip360_sat.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mip360_sat.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://ffushiyang.github.io/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://ffushiyang.github.io/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/zBSH-k9GbV4">video</a>
              <p></p>
              <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
            </td>
          </tr>

   
<!--2023.7.14-0:10我在这里删除了很多代码-->
     
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fast_texture.jpg" alt="fast-texture" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing">
                <papertitle>Discovering Efficiency in Coarse-To-Fine Texture Classification</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>Technical Report</em>, 2010
              <br>
              <a href="data/BarronTR2010.bib">bibtex</a>
              <p>A model and feature representation that allows for sub-linear coarse-to-fine semantic segmentation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/prl.jpg" alt="prl" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                <papertitle>Parallelizing Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>
              <br>
              <em>Technical Report</em>, 2009
              <br>
              <a href="data/BarronPRL2009.bib">bibtex</a>
              <p>Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bd_promo.jpg" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                <papertitle>Blind Date: Using Proper Motions to Determine the Ages of Historical Images</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              <br>
              <em>The Astronomical Journal</em>, 136, 2008
              <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              <br>
              <em>The Astronomical Journal</em>, 135, 2008
              <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
              <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
			  <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <!--
		      Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              	-->
		<!--
		      <strong>&copy;FU Shi-yang.<strong> Last updated: 13th July, 2023.
		-->
		You probably want this website template, thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. <br>
		Last updated: 13th July, 2023.
		
	      </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
